{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets icecream accelerate trl --no-cache-dir -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from icecream import ic\n",
    "\n",
    "# Carregar o modelo e o tokenizador GPT-2\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Definir um token de preenchimento\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ic.configureOutput(prefix=\"Model confgi\")\n",
    "ic(model)\n",
    "\n",
    "# Carregar o dataset de diÃ¡logos\n",
    "#dataset = load_dataset(\"Osondu/reddit_autism_dataset\", trust_remote_code=True )\n",
    "#dataset_autism = load_dataset(\"Osondu/reddit_autism_dataset\", split=\"train\" )\n",
    "# Verificar a estrutura do dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batches_train():\n",
    "    dataset = load_dataset(\"Osondu/reddit_autism_dataset\",  streaming=True,  split=\"train\")\n",
    "    ic.configureOutput(prefix=\"Dataset autism\\n\")\n",
    "    ic(dataset)\n",
    "\n",
    "    total_samples = 800\n",
    "    val_pct = 0.1\n",
    "    train_limit = int(total_samples * (1 - val_pct))\n",
    "    counter = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        if counter >= train_limit:\n",
    "            break\n",
    "\n",
    "        inst = row[\"instructions\"]\n",
    "        input = row[\"input\"]\n",
    "        output = row[\"output\"]\n",
    "\n",
    "        chat_template = f\"<s>[INST] {inst}\\nInput: {input} [/INST] {output}</s>\"\n",
    "        tokenized_output = tokenizer(chat_template)\n",
    "        yield {'text': chat_template}\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "def gen_batches_val():\n",
    "    dataset = load_dataset(\"Osondu/reddit_autism_dataset\",  streaming=True,  split=\"train\")\n",
    "    total_samples = 800\n",
    "    val_pct = 0.1\n",
    "    train_limit = int(total_samples * (1 - val_pct))\n",
    "    counter = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        if counter >= total_samples:\n",
    "           break\n",
    "\n",
    "        inst = row[\"instructions\"]\n",
    "        input = row[\"input\"]\n",
    "        output = row[\"output\"]\n",
    "\n",
    "        chat_template = f\"<s>[INST] {inst}\\nInput: {input} [/INST] {output}</s>\"\n",
    "        tokenized_output = tokenizer(chat_template)\n",
    "        yield {'text': chat_template}\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
